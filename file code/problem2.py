# -*- coding: utf-8 -*-
"""problem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FXaOleCXPnGkANk3w4NJTsHBJA8FxAT3

**問題2**:      IMDbのデータセットに対してより高い精度を実現するプログラム(PyTorch)を作成せよ。ただし、プログラムは第12回の講義資料のプログラム(もしくはLab Work (5)で作成したプログラム)を改良して作成せよ。その「プログラム」と「実行結果」およびそれらに関する「解説」   をwordファイルにまとめて提出せよ。また、プログラムのソースコード(.py)も提出せよ。例: LSTM+Dropout, ハイパーパラメータ調整など期待される精度: 75%以上

**Problem 2**: Write a program (PyTorch) that achieves higher accuracy on the IMDb data set. The program should be an improved version of the program in the 12th lecture (or the program you wrote in Lab Work (5)).  Submit  the  “Program”,  its  “Execution Results”,  and  an  “Explanation”  of  them  in  a  word  file.  Also submit the source code (.py) of the program.Example: LSTM+Dropout, hyperparameter tuning, etc.Expected accuracy: 75% or more
"""

# !pip install portalocker
# !pip install torchdata
# !pip install torch
# !pip install torchtext

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
import numpy as np
import random
import math
import time
import pandas as pd
import torchtext

train_iter, test_iter = torchtext.datasets.IMDB(split=('train', 'test'))
tokenizer = torchtext.data.utils.get_tokenizer('basic_english')


MODELNAME = "imdb-rnn.model"
BATCHSIZE = 64
LR = 1e-5
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(DEVICE)

train_data = [(label, tokenizer(line)) for label, line in train_iter]
train_data.sort(key = lambda x: len(x[1]))
test_data = [(label, tokenizer(line)) for label, line in test_iter]
test_data.sort(key = lambda x: len(x[1]))

for i in range(5):
  print(train_data[i])

def make_vocab(train_data, min_freq):
  vocab = {}
  for label, tokenlist in train_data:
    for token in tokenlist:
      if token not in vocab:
        vocab[token] = 0
      vocab[token] += 1
  vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>', 0)]
  vocabidx = {}
  for token, freq in vocab.items():
    if freq >= min_freq:
      idx = len(vocablist)
      vocablist.append((token, freq))
      vocabidx[token] = idx
  vocabidx['<unk>'] = 0
  vocabidx['<pad>'] = 1
  vocabidx['<cls>'] = 2
  vocabidx['<eos>'] = 3
  return vocablist, vocabidx


vocablist, vocabidx = make_vocab(train_data, 10)

def preprocess(data, vocabidx):
  rr = []
  for label, tokenlist in data:
    tkl = ['<cls>']
    for token in tokenlist:
      tkl.append(token if token in vocabidx else '<unk>')
    tkl.append('<eos>')
    rr.append((label, tkl))
  return rr

train_data = preprocess(train_data, vocabidx)
test_data = preprocess(test_data, vocabidx)
for i in range(5):
  print(train_data[i])

def make_batch(data, batchsize):
  bb = []
  blabel = []
  btokenlist = []
  for label, tokenlist in data:
    blabel.append(label)
    btokenlist.append(tokenlist)
    if len(blabel) >= batchsize:
      bb.append((btokenlist, blabel))
      blabel = []
      btokenlist = []
  if len(blabel) > 0:
    bb.append((btokenlist, blabel))
  return bb

train_data = make_batch(train_data, BATCHSIZE)
test_data = make_batch(test_data, BATCHSIZE)
for i in range(5):
  print(train_data[i])

def padding(bb):
  for tokenlists,labels in bb:
    maxlen = max([len(x) for x in tokenlists])
    for tkl in tokenlists:
      for i in range(maxlen - len(tkl)):
        tkl.append('<pad>')
  return bb

train_data=padding(train_data)
test_data = padding(test_data)

for i in range(5):
  print(train_data[i])

def word2id(bb,vocabidx):
  rr=[]
  for tokenlists, labels in bb:
    id_labels = [label - 1 for label in labels]
    id_tokenlists = []
    for tokenlist in tokenlists:
      id_tokenlists.append([vocabidx[token] for token in tokenlist])
    rr.append((id_tokenlists,id_labels))
  return rr

train_data = word2id(train_data,vocabidx)
test_data = word2id(test_data,vocabidx)
for i in range(5):
  print(train_data[i])

class MyGRU(torch.nn.Module):
    def __init__(self, vocab_size):
        super(MyGRU, self).__init__()
        embedding_dim = 300
        hidden_dim = 128
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 2)

    def forward(self, x):
        embedded = self.embedding(x)
        _, hidden = self.rnn(embedded)
        output = self.fc(hidden[-1])
        return output

def train(EPOCH):
    model = MyGRU(len(vocablist)).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(EPOCH):
        total_loss = 0
        correct = 0
        total = 0

        model.train()
        for tokenlists, labels in train_data:
            tokenlists = torch.tensor(tokenlists, dtype=torch.long).to(DEVICE)
            labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)

            optimizer.zero_grad()
            outputs = model(tokenlists)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        print("Epoch:", epoch+1, "Loss:", total_loss, "Train Accuracy:", (correct / total))
    torch.save(model.state_dict(), MODELNAME)

def test():
    model = MyGRU(len(vocablist)).to(DEVICE)
    model.load_state_dict(torch.load(MODELNAME))
    model.eval()

    correct = 0
    total = 0

    with torch.no_grad():
        for tokenlists, labels in test_data:
            tokenlists = torch.tensor(tokenlists, dtype=torch.long).to(DEVICE)
            labels = torch.tensor(labels, dtype=torch.long).to(DEVICE)

            outputs = model(tokenlists)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print("correct:", correct)
    print("total:", total)
    print("Test Accuracy:", (correct / total))

train(10)
test()

train(20)
test()

train(50)
test()